{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bad Apple SIREN Training (H=32) with Weight Continuity\n\nTrain 658 SIREN neural networks (3->32->32->3) for Bad Apple video playback on Zynq Z7020 FPGA.\n\n**Weight continuity regularization**: Consecutive segments are initialized from the same random\nweights and trained with an L2 penalty on weight differences. This keeps segments close in\nweight space, enabling smooth alpha-blended morphing on the FPGA (rather than hard cuts).\n\n**Before running:** Make sure runtime is set to **A100 GPU** (Runtime -> Change runtime type -> A100)\n\n**Steps:**\n1. Upload `source.mp4` when prompted\n2. Frames get extracted automatically\n3. Training runs (~20-30 min on A100)\n4. Weights are exported as `weights.bin` (single concatenated file) and zipped for download"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Upload source video and extract frames\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Upload source.mp4\n",
    "print(\"Upload source.mp4 (Bad Apple, 320x172, 7MB)\")\n",
    "uploaded = files.upload()\n",
    "assert 'source.mp4' in uploaded, f\"Expected source.mp4, got {list(uploaded.keys())}\"\n",
    "print(f\"Uploaded {len(uploaded['source.mp4'])} bytes\")\n",
    "\n",
    "# Extract frames\n",
    "os.makedirs('frames', exist_ok=True)\n",
    "!ffmpeg -i source.mp4 -vf \"scale=320:172\" frames/frame_%05d.png -y -loglevel warning\n",
    "n_frames = len([f for f in os.listdir('frames') if f.endswith('.png')])\n",
    "print(f\"Extracted {n_frames} frames\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 2: Batched SIREN training with weight continuity regularization\nimport math, os, struct, time\nimport numpy as np\nimport torch\nfrom pathlib import Path\nfrom PIL import Image\n\n# =========================================================\n# Constants\n# =========================================================\nFRAME_DIR = Path('frames')\nWEIGHTS_DIR = Path('weights')\nTOTAL_FRAMES = len([f for f in os.listdir('frames') if f.endswith('.png')])\nFRAME_W, FRAME_H = 320, 172\nASPECT_Y = FRAME_H / FRAME_W  # 0.5375\nFRAMES_PER_SEGMENT = 10\nN_SEGMENTS = (TOTAL_FRAMES + FRAMES_PER_SEGMENT - 1) // FRAMES_PER_SEGMENT\nFRAC_BITS = 28\nQ_SCALE = 1 << FRAC_BITS\nHIDDEN = 32\nOMEGA_0 = 10.0\n\n# Training hyperparameters\nEPOCHS = 5000\nLR = 1e-4\nSAMPLES = 50000\nMINI_BATCH = 10000\nEVAL_EVERY = 50\n\n# Weight continuity: L2 penalty on differences between consecutive segments.\n# Keeps segments close in weight space for smooth FPGA alpha-blending.\n# 0.0 = independent segments (hard cuts only)\n# 0.1 = moderate continuity (good starting point)\n# 1.0 = very smooth transitions, lower per-segment quality\nREG_LAMBDA = 0.1\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")\nif device == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\nprint(f\"Config: {TOTAL_FRAMES} frames, {N_SEGMENTS} segments, H={HIDDEN}\")\nprint(f\"Training: {EPOCHS} epochs, {SAMPLES} samples/seg, mini-batch {MINI_BATCH}\")\nprint(f\"Weight continuity: lambda={REG_LAMBDA}\")\n\n# =========================================================\n# Load ALL frame data into a single GPU tensor\n# =========================================================\nprint(f\"\\nLoading {TOTAL_FRAMES} frames into GPU...\")\nt0 = time.time()\n\nall_seg_frames = []\nfor seg in range(N_SEGMENTS):\n    start_frame = seg * FRAMES_PER_SEGMENT + 1\n    end_frame = min(start_frame + FRAMES_PER_SEGMENT, TOTAL_FRAMES + 1)\n    frames = []\n    for i in range(start_frame, end_frame):\n        path = FRAME_DIR / f\"frame_{i:05d}.png\"\n        if not path.exists():\n            break\n        arr = np.array(Image.open(path).convert('L'), dtype=np.float32) / 127.5 - 1.0\n        frames.append(arr)\n    if not frames:\n        break\n    while len(frames) < FRAMES_PER_SEGMENT:\n        frames.append(frames[-1])\n    all_seg_frames.append(np.stack(frames))\n\nn_seg = len(all_seg_frames)\nframes_gpu = torch.from_numpy(np.stack(all_seg_frames)).to(device)\ndel all_seg_frames\n\nx_coords = (torch.arange(FRAME_W, device=device, dtype=torch.float32) / (FRAME_W - 1)) * 2.0 - 1.0\ny_coords = (torch.arange(FRAME_H, device=device, dtype=torch.float32) / (FRAME_H - 1)) * 2.0 * ASPECT_Y - ASPECT_Y\nt_coords = (torch.arange(FRAMES_PER_SEGMENT, device=device, dtype=torch.float32) / max(FRAMES_PER_SEGMENT - 1, 1)) * 2.0 - 1.0\n\nprint(f\"Loaded {n_seg} segments in {time.time()-t0:.1f}s\")\nprint(f\"  frames_gpu: {frames_gpu.shape} = {frames_gpu.element_size() * frames_gpu.nelement() / 1e6:.0f} MB on GPU\")\n\n\n# =========================================================\n# GPU-native sampling\n# =========================================================\ndef sample_gpu(frames_gpu, n_seg, samples_per_seg):\n    fi = torch.randint(0, FRAMES_PER_SEGMENT, (n_seg, samples_per_seg), device=device)\n    yi = torch.randint(0, FRAME_H, (n_seg, samples_per_seg), device=device)\n    xi = torch.randint(0, FRAME_W, (n_seg, samples_per_seg), device=device)\n    x = x_coords[xi]\n    y = y_coords[yi]\n    t = t_coords[fi]\n    coords = torch.stack([x, y, t], dim=2)\n    seg_idx = torch.arange(n_seg, device=device).unsqueeze(1).expand_as(fi)\n    targets = frames_gpu[seg_idx, fi, yi, xi]\n    targets_3ch = targets.unsqueeze(2).expand(-1, -1, 3)\n    return coords, targets_3ch\n\n\n# =========================================================\n# Batched SIREN with shared initialization\n# =========================================================\ndef init_weights_shared(n_seg, hidden, omega_0, device):\n    \"\"\"Initialize all segments from the SAME random weights.\n\n    Shared init means segments start identical and only diverge as needed\n    to fit their individual frame data. Combined with continuity regularization,\n    this keeps consecutive segments close in weight space.\n    \"\"\"\n    W1_base = torch.empty(1, hidden, 3, device=device).uniform_(-1.0 / 3, 1.0 / 3)\n    W1 = W1_base.expand(n_seg, -1, -1).clone()\n    b1 = torch.zeros(n_seg, 1, hidden, device=device)\n\n    bound2 = math.sqrt(6.0 / hidden) / omega_0\n    W2_base = torch.empty(1, hidden, hidden, device=device).uniform_(-bound2, bound2)\n    W2 = W2_base.expand(n_seg, -1, -1).clone()\n    b2 = torch.zeros(n_seg, 1, hidden, device=device)\n\n    bound3 = math.sqrt(6.0 / hidden) / omega_0\n    W3_base = torch.empty(1, 3, hidden, device=device).uniform_(-bound3, bound3)\n    W3 = W3_base.expand(n_seg, -1, -1).clone()\n    b3 = torch.zeros(n_seg, 1, 3, device=device)\n\n    params = [W1, b1, W2, b2, W3, b3]\n    for p in params:\n        p.requires_grad_(True)\n    return params\n\n\ndef batched_forward(coords, params, omega_0):\n    W1, b1, W2, b2, W3, b3 = params\n    h = torch.bmm(coords, W1.transpose(1, 2)) + b1\n    h = torch.sin(omega_0 * h)\n    h = torch.bmm(h, W2.transpose(1, 2)) + b2\n    h = torch.sin(omega_0 * h)\n    out = torch.bmm(h, W3.transpose(1, 2)) + b3\n    return torch.sin(out)\n\n\ndef weight_continuity_loss(params):\n    \"\"\"L2 penalty on weight differences between consecutive segments.\n\n    Returns mean squared difference across all parameters and all\n    consecutive segment pairs. This is the key term that enables\n    smooth alpha-blended morphing between segments on FPGA.\n    \"\"\"\n    loss = torch.tensor(0.0, device=params[0].device)\n    for p in params:\n        diff = p[1:] - p[:-1]  # (N_SEG-1, ...)\n        loss = loss + (diff ** 2).mean()\n    return loss\n\n\n# =========================================================\n# Q4.28 export\n# =========================================================\ndef float_to_q428(val):\n    clamped = max(-8.0, min(val, 8.0 - 1.0 / Q_SCALE))\n    raw = int(round(clamped * Q_SCALE))\n    if raw < 0:\n        raw = raw & 0xFFFFFFFF\n    return raw\n\n\ndef export_segment_q428(params, seg_idx, omega_0):\n    \"\"\"Export one segment as list of Q4.28 u32 values.\"\"\"\n    W1, b1, W2, b2, W3, b3 = params\n    layers = [\n        (W1[seg_idx].detach().cpu().numpy() * omega_0,\n         b1[seg_idx, 0].detach().cpu().numpy() * omega_0),\n        (W2[seg_idx].detach().cpu().numpy() * omega_0,\n         b2[seg_idx, 0].detach().cpu().numpy() * omega_0),\n        (W3[seg_idx].detach().cpu().numpy(),\n         b3[seg_idx, 0].detach().cpu().numpy()),\n    ]\n    all_vals = []\n    for weights, biases in layers:\n        for j in range(weights.shape[0]):\n            for k in range(weights.shape[1]):\n                all_vals.append(float_to_q428(weights[j, k]))\n        for j in range(biases.shape[0]):\n            all_vals.append(float_to_q428(biases[j]))\n    return all_vals\n\n\ndef export_segment_pt(params, seg_idx, omega_0):\n    W1, b1, W2, b2, W3, b3 = params\n    state_dict = {\n        'layers.0.linear.weight': W1[seg_idx].detach().cpu(),\n        'layers.0.linear.bias': b1[seg_idx, 0].detach().cpu(),\n        'layers.1.linear.weight': W2[seg_idx].detach().cpu(),\n        'layers.1.linear.bias': b2[seg_idx, 0].detach().cpu(),\n        'output_layer.weight': W3[seg_idx].detach().cpu(),\n        'output_layer.bias': b3[seg_idx, 0].detach().cpu(),\n    }\n    pt_path = WEIGHTS_DIR / f\"segment_{seg_idx:03d}.pt\"\n    torch.save(state_dict, pt_path)\n    return pt_path\n\n\n# =========================================================\n# Evaluation (GPU-native)\n# =========================================================\ndef evaluate_psnr(params, frames_gpu, omega_0, device, max_segs=None):\n    W1, b1, W2, b2, W3, b3 = params\n    n_seg = frames_gpu.shape[0] if max_segs is None else min(max_segs, frames_gpu.shape[0])\n    H, W = FRAME_H, FRAME_W\n    xx = x_coords.unsqueeze(0).expand(H, -1)\n    yy = y_coords.unsqueeze(1).expand(-1, W)\n    xy_flat = torch.stack([xx.reshape(-1), yy.reshape(-1)], dim=1)\n    n_pixels = H * W\n    psnrs = []\n    chunk = 64\n    for c_start in range(0, n_seg, chunk):\n        c_end = min(c_start + chunk, n_seg)\n        c_size = c_end - c_start\n        seg_mse_sum = torch.zeros(c_size, device=device)\n        for fi in range(FRAMES_PER_SEGMENT):\n            t_val = t_coords[fi]\n            tt = torch.full((n_pixels, 1), t_val, device=device)\n            coords = torch.cat([xy_flat, tt], dim=1)\n            coords = coords.unsqueeze(0).expand(c_size, -1, -1)\n            with torch.no_grad():\n                p = [W1[c_start:c_end], b1[c_start:c_end],\n                     W2[c_start:c_end], b2[c_start:c_end],\n                     W3[c_start:c_end], b3[c_start:c_end]]\n                pred = batched_forward(coords, p, omega_0)\n            pred_gray = pred[:, :, 0].reshape(c_size, H, W).clamp(-1.0, 1.0)\n            gt = frames_gpu[c_start:c_end, fi]\n            seg_mse_sum += ((pred_gray - gt) ** 2).mean(dim=(1, 2))\n        seg_mse_avg = seg_mse_sum / FRAMES_PER_SEGMENT\n        seg_psnr = 10 * torch.log10(4.0 / seg_mse_avg)\n        for s in range(c_size):\n            psnrs.append((c_start + s, seg_psnr[s].item()))\n    return psnrs\n\n\n# =========================================================\n# Main training loop\n# =========================================================\nWEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nInitializing {n_seg} SIREN networks (3->{HIDDEN}->{HIDDEN}->3)...\")\nprint(f\"Using shared initialization (all segments start identical)\")\nparams = init_weights_shared(n_seg, HIDDEN, OMEGA_0, device)\nW1, b1, W2, b2, W3, b3 = params\n\noptimizer = torch.optim.Adam(params, lr=LR)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=EPOCHS, eta_min=LR * 0.01)\n\n# Track global best (not per-segment, since regularization links segments)\nbest_total_loss = float('inf')\nbest_params = [p.detach().clone() for p in params]\n\nn_mini = max(1, SAMPLES // MINI_BATCH)\nprint(f\"Training: {EPOCHS} epochs, {SAMPLES} samples/seg, {n_mini} mini-batches\")\nprint(f\"Weight continuity: lambda={REG_LAMBDA}\")\nprint()\n\nt_train = time.time()\nfor epoch in range(EPOCHS):\n    coords_all, targets_all = sample_gpu(frames_gpu, n_seg, SAMPLES)\n\n    epoch_mse = 0.0\n    epoch_reg = 0.0\n    perm = torch.randperm(SAMPLES, device=device)\n\n    for mb in range(n_mini):\n        start = mb * MINI_BATCH\n        end = min(start + MINI_BATCH, SAMPLES)\n        idx = perm[start:end]\n        batch_coords = coords_all[:, idx]\n        batch_targets = targets_all[:, idx]\n        pred = batched_forward(batch_coords, params, OMEGA_0)\n        mse_loss = ((pred - batch_targets) ** 2).mean()\n        reg_loss = weight_continuity_loss(params)\n        loss = mse_loss + REG_LAMBDA * reg_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_mse += mse_loss.item()\n        epoch_reg += reg_loss.item()\n\n    scheduler.step()\n    avg_mse = epoch_mse / n_mini\n    avg_reg = epoch_reg / n_mini\n    avg_total = avg_mse + REG_LAMBDA * avg_reg\n\n    # Track global best\n    if avg_total < best_total_loss:\n        best_total_loss = avg_total\n        for i, p in enumerate(params):\n            best_params[i].copy_(p.detach())\n\n    if (epoch + 1) % EVAL_EVERY == 0 or epoch == 0:\n        elapsed = time.time() - t_train\n        remaining = elapsed / (epoch + 1) * (EPOCHS - epoch - 1)\n        print(f\"epoch {epoch+1:5d}/{EPOCHS}: mse={avg_mse:.6f} reg={avg_reg:.6f} \"\n              f\"total={avg_total:.6f} lr={scheduler.get_last_lr()[0]:.2e} \"\n              f\"[{elapsed:.0f}s, ~{remaining:.0f}s left]\")\n        if (epoch + 1) % 500 == 0:\n            psnrs = evaluate_psnr(params, frames_gpu, OMEGA_0, device, max_segs=5)\n            avg_psnr = np.mean([p for _, p in psnrs])\n            print(f\"  PSNR (first 5 segs): {avg_psnr:.1f} dB\")\n\ntotal_time = time.time() - t_train\nprint(f\"\\nTraining done in {total_time:.0f}s ({total_time/60:.1f} min)\")\n\n# Restore best weights\nfor i, p in enumerate(params):\n    p.data.copy_(best_params[i])\n\n# Report weight continuity stats\nwith torch.no_grad():\n    final_reg = weight_continuity_loss(params).item()\n    # Per-param max diff between consecutive segments\n    max_diffs = []\n    for p in params:\n        max_diffs.append((p[1:] - p[:-1]).abs().max().item())\n    print(f\"Final continuity loss: {final_reg:.6f}\")\n    print(f\"Max weight diff between consecutive segments: {max(max_diffs):.4f}\")\n\n# Full PSNR evaluation\nprint(\"\\nEvaluating all segments...\")\nt_eval = time.time()\npsnrs = evaluate_psnr(params, frames_gpu, OMEGA_0, device)\neval_time = time.time() - t_eval\nall_psnr = [p for _, p in psnrs]\nprint(f\"PSNR: avg={np.mean(all_psnr):.1f}dB \"\n      f\"min={np.min(all_psnr):.1f}dB max={np.max(all_psnr):.1f}dB \"\n      f\"[{eval_time:.0f}s]\")\n\n# Export all segments as concatenated weights.bin + individual .pt files\nprint(f\"\\nExporting {n_seg} segments...\")\nt_export = time.time()\nwith open(WEIGHTS_DIR / 'weights.bin', 'wb') as f_concat:\n    for seg in range(n_seg):\n        export_segment_pt(params, seg, OMEGA_0)\n        vals = export_segment_q428(params, seg, OMEGA_0)\n        for val in vals:\n            f_concat.write(struct.pack('<I', val))\nexport_size = os.path.getsize(WEIGHTS_DIR / 'weights.bin')\nprint(f\"Exported in {time.time()-t_export:.1f}s\")\nprint(f\"  weights.bin: {export_size} bytes ({export_size/1e6:.1f} MB)\")\nprint(f\"  .pt files: {WEIGHTS_DIR}/segment_*.pt\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Total: {n_seg} segments, {EPOCHS} epochs, lambda={REG_LAMBDA}\")\nprint(f\"Time: {total_time:.0f}s training + {eval_time:.0f}s eval\")\nprint(f\"PSNR: {np.mean(all_psnr):.1f} dB average\")\nprint(f\"Continuity: {final_reg:.6f} (max diff: {max(max_diffs):.4f})\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 3: Download weights.bin (single file for FPGA)\nimport os\nfrom google.colab import files\n\nweights_path = 'weights/weights.bin'\nsize_mb = os.path.getsize(weights_path) / 1e6\nprint(f\"weights.bin: {size_mb:.1f} MB\")\nprint(f\"Copy to SD card root alongside BOOT.bin\")\n\nfiles.download(weights_path)",
   "execution_count": null,
   "outputs": []
  }
 ]
}