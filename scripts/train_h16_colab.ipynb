{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Bad Apple SIREN Training (H=16)\n\nTrain 658 SIREN neural networks (3→16→16→3) for Bad Apple video playback on Zynq Z7020 FPGA.\n\n**Before running:** Make sure runtime is set to **A100 GPU** (Runtime → Change runtime type → A100)\n\n**Steps:**\n1. Upload `source.mp4` when prompted\n2. Frames get extracted automatically\n3. Training runs (~10-15 min on A100)\n4. Weights are exported and zipped for download"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Upload source video and extract frames\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Upload source.mp4\n",
    "print(\"Upload source.mp4 (Bad Apple, 320x172, 7MB)\")\n",
    "uploaded = files.upload()\n",
    "assert 'source.mp4' in uploaded, f\"Expected source.mp4, got {list(uploaded.keys())}\"\n",
    "print(f\"Uploaded {len(uploaded['source.mp4'])} bytes\")\n",
    "\n",
    "# Extract frames\n",
    "os.makedirs('frames', exist_ok=True)\n",
    "!ffmpeg -i source.mp4 -vf \"scale=320:172\" frames/frame_%05d.png -y -loglevel warning\n",
    "n_frames = len([f for f in os.listdir('frames') if f.endswith('.png')])\n",
    "print(f\"Extracted {n_frames} frames\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 2: Batched SIREN training — GPU-native sampling, no CPU bottleneck\nimport math, os, struct, time\nimport numpy as np\nimport torch\nfrom pathlib import Path\nfrom PIL import Image\n\n# =========================================================\n# Constants\n# =========================================================\nFRAME_DIR = Path('frames')\nWEIGHTS_DIR = Path('weights')\nTOTAL_FRAMES = len([f for f in os.listdir('frames') if f.endswith('.png')])\nFRAME_W, FRAME_H = 320, 172\nASPECT_Y = FRAME_H / FRAME_W  # 0.5375\nFRAMES_PER_SEGMENT = 10\nN_SEGMENTS = (TOTAL_FRAMES + FRAMES_PER_SEGMENT - 1) // FRAMES_PER_SEGMENT\nFRAC_BITS = 28\nQ_SCALE = 1 << FRAC_BITS\nHIDDEN = 16\nOMEGA_0 = 10.0\n\n# Training hyperparameters\nEPOCHS = 5000\nLR = 1e-4\nSAMPLES = 50000\nMINI_BATCH = 10000\nEVAL_EVERY = 50\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {device}\")\nif device == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\nprint(f\"Config: {TOTAL_FRAMES} frames, {N_SEGMENTS} segments, H={HIDDEN}\")\nprint(f\"Training: {EPOCHS} epochs, {SAMPLES} samples/seg, mini-batch {MINI_BATCH}\")\n\n# =========================================================\n# Load ALL frame data into a single GPU tensor\n# =========================================================\nprint(f\"\\nLoading {TOTAL_FRAMES} frames into GPU...\")\nt0 = time.time()\n\n# frames_gpu: (N_SEGMENTS, FRAMES_PER_SEGMENT, H, W) on GPU\nall_seg_frames = []\nfor seg in range(N_SEGMENTS):\n    start_frame = seg * FRAMES_PER_SEGMENT + 1\n    end_frame = min(start_frame + FRAMES_PER_SEGMENT, TOTAL_FRAMES + 1)\n    frames = []\n    for i in range(start_frame, end_frame):\n        path = FRAME_DIR / f\"frame_{i:05d}.png\"\n        if not path.exists():\n            break\n        arr = np.array(Image.open(path).convert('L'), dtype=np.float32) / 127.5 - 1.0\n        frames.append(arr)\n    if not frames:\n        break\n    while len(frames) < FRAMES_PER_SEGMENT:\n        frames.append(frames[-1])\n    all_seg_frames.append(np.stack(frames))\n\nn_seg = len(all_seg_frames)\nframes_gpu = torch.from_numpy(np.stack(all_seg_frames)).to(device)  # (N, F, H, W)\ndel all_seg_frames  # free CPU memory\n\n# Pre-compute coordinate mappings on GPU\nx_coords = (torch.arange(FRAME_W, device=device, dtype=torch.float32) / (FRAME_W - 1)) * 2.0 - 1.0\ny_coords = (torch.arange(FRAME_H, device=device, dtype=torch.float32) / (FRAME_H - 1)) * 2.0 * ASPECT_Y - ASPECT_Y\nt_coords = (torch.arange(FRAMES_PER_SEGMENT, device=device, dtype=torch.float32) / max(FRAMES_PER_SEGMENT - 1, 1)) * 2.0 - 1.0\n\nprint(f\"Loaded {n_seg} segments in {time.time()-t0:.1f}s\")\nprint(f\"  frames_gpu: {frames_gpu.shape} = {frames_gpu.element_size() * frames_gpu.nelement() / 1e6:.0f} MB on GPU\")\n\n\n# =========================================================\n# GPU-native sampling — no numpy, no CPU-GPU transfer\n# =========================================================\ndef sample_gpu(frames_gpu, n_seg, samples_per_seg):\n    \"\"\"Sample training data entirely on GPU. Returns coords (N,S,3), targets (N,S,3).\"\"\"\n    fi = torch.randint(0, FRAMES_PER_SEGMENT, (n_seg, samples_per_seg), device=device)\n    yi = torch.randint(0, FRAME_H, (n_seg, samples_per_seg), device=device)\n    xi = torch.randint(0, FRAME_W, (n_seg, samples_per_seg), device=device)\n\n    x = x_coords[xi]       # (N, S)\n    y = y_coords[yi]       # (N, S)\n    t = t_coords[fi]       # (N, S)\n    coords = torch.stack([x, y, t], dim=2)  # (N, S, 3)\n\n    # Gather pixel values: frames_gpu is (N, F, H, W)\n    seg_idx = torch.arange(n_seg, device=device).unsqueeze(1).expand_as(fi)  # (N, S)\n    targets = frames_gpu[seg_idx, fi, yi, xi]  # (N, S)\n    targets_3ch = targets.unsqueeze(2).expand(-1, -1, 3)  # (N, S, 3)\n\n    return coords, targets_3ch\n\n\n# =========================================================\n# Batched SIREN\n# =========================================================\ndef init_weights(n_seg, hidden, omega_0, device):\n    W1 = torch.empty(n_seg, hidden, 3, device=device)\n    W1.uniform_(-1.0 / 3, 1.0 / 3)\n    b1 = torch.zeros(n_seg, 1, hidden, device=device)\n    bound2 = math.sqrt(6.0 / hidden) / omega_0\n    W2 = torch.empty(n_seg, hidden, hidden, device=device)\n    W2.uniform_(-bound2, bound2)\n    b2 = torch.zeros(n_seg, 1, hidden, device=device)\n    bound3 = math.sqrt(6.0 / hidden) / omega_0\n    W3 = torch.empty(n_seg, 3, hidden, device=device)\n    W3.uniform_(-bound3, bound3)\n    b3 = torch.zeros(n_seg, 1, 3, device=device)\n    params = [W1, b1, W2, b2, W3, b3]\n    for p in params:\n        p.requires_grad_(True)\n    return params\n\n\ndef batched_forward(coords, params, omega_0):\n    W1, b1, W2, b2, W3, b3 = params\n    h = torch.bmm(coords, W1.transpose(1, 2)) + b1\n    h = torch.sin(omega_0 * h)\n    h = torch.bmm(h, W2.transpose(1, 2)) + b2\n    h = torch.sin(omega_0 * h)\n    out = torch.bmm(h, W3.transpose(1, 2)) + b3\n    return torch.sin(out)\n\n\n# =========================================================\n# Q4.28 export\n# =========================================================\ndef float_to_q428(val):\n    clamped = max(-8.0, min(val, 8.0 - 1.0 / Q_SCALE))\n    raw = int(round(clamped * Q_SCALE))\n    if raw < 0:\n        raw = raw & 0xFFFFFFFF\n    return raw\n\n\ndef export_segment_binary(params, seg_idx, omega_0):\n    W1, b1, W2, b2, W3, b3 = params\n    layers = [\n        (W1[seg_idx].detach().cpu().numpy() * omega_0,\n         b1[seg_idx, 0].detach().cpu().numpy() * omega_0),\n        (W2[seg_idx].detach().cpu().numpy() * omega_0,\n         b2[seg_idx, 0].detach().cpu().numpy() * omega_0),\n        (W3[seg_idx].detach().cpu().numpy(),\n         b3[seg_idx, 0].detach().cpu().numpy()),\n    ]\n    all_vals = []\n    for weights, biases in layers:\n        for j in range(weights.shape[0]):\n            for k in range(weights.shape[1]):\n                all_vals.append(float_to_q428(weights[j, k]))\n        for j in range(biases.shape[0]):\n            all_vals.append(float_to_q428(biases[j]))\n    bin_path = WEIGHTS_DIR / f\"segment_{seg_idx:03d}.bin\"\n    with open(bin_path, 'wb') as f:\n        for val in all_vals:\n            f.write(struct.pack('<I', val))\n    return bin_path\n\n\ndef export_segment_pt(params, seg_idx, omega_0):\n    W1, b1, W2, b2, W3, b3 = params\n    state_dict = {\n        'layers.0.linear.weight': W1[seg_idx].detach().cpu(),\n        'layers.0.linear.bias': b1[seg_idx, 0].detach().cpu(),\n        'layers.1.linear.weight': W2[seg_idx].detach().cpu(),\n        'layers.1.linear.bias': b2[seg_idx, 0].detach().cpu(),\n        'output_layer.weight': W3[seg_idx].detach().cpu(),\n        'output_layer.bias': b3[seg_idx, 0].detach().cpu(),\n    }\n    pt_path = WEIGHTS_DIR / f\"segment_{seg_idx:03d}.pt\"\n    torch.save(state_dict, pt_path)\n    return pt_path\n\n\n# =========================================================\n# Evaluation (GPU-native)\n# =========================================================\ndef evaluate_psnr(params, frames_gpu, omega_0, device, max_segs=None):\n    W1, b1, W2, b2, W3, b3 = params\n    n_seg = frames_gpu.shape[0] if max_segs is None else min(max_segs, frames_gpu.shape[0])\n    H, W = FRAME_H, FRAME_W\n\n    # Pre-compute full coordinate grid on GPU\n    xx = x_coords.unsqueeze(0).expand(H, -1)  # (H, W)\n    yy = y_coords.unsqueeze(1).expand(-1, W)  # (H, W)\n    xy_flat = torch.stack([xx.reshape(-1), yy.reshape(-1)], dim=1)  # (H*W, 2)\n    n_pixels = H * W\n\n    psnrs = []\n    chunk = 64\n    for c_start in range(0, n_seg, chunk):\n        c_end = min(c_start + chunk, n_seg)\n        c_size = c_end - c_start\n        seg_mse_sum = torch.zeros(c_size, device=device)\n\n        for fi in range(FRAMES_PER_SEGMENT):\n            t_val = t_coords[fi]\n            tt = torch.full((n_pixels, 1), t_val, device=device)\n            coords = torch.cat([xy_flat, tt], dim=1)  # (H*W, 3)\n            coords = coords.unsqueeze(0).expand(c_size, -1, -1)  # (chunk, H*W, 3)\n\n            with torch.no_grad():\n                p = [W1[c_start:c_end], b1[c_start:c_end],\n                     W2[c_start:c_end], b2[c_start:c_end],\n                     W3[c_start:c_end], b3[c_start:c_end]]\n                pred = batched_forward(coords, p, omega_0)  # (chunk, H*W, 3)\n\n            pred_gray = pred[:, :, 0].reshape(c_size, H, W)\n            pred_gray = pred_gray.clamp(-1.0, 1.0)\n            gt = frames_gpu[c_start:c_end, fi]  # (chunk, H, W)\n            seg_mse_sum += ((pred_gray - gt) ** 2).mean(dim=(1, 2))\n\n        seg_mse_avg = seg_mse_sum / FRAMES_PER_SEGMENT\n        seg_psnr = 10 * torch.log10(4.0 / seg_mse_avg)\n        for s in range(c_size):\n            psnrs.append((c_start + s, seg_psnr[s].item()))\n\n    return psnrs\n\n\n# =========================================================\n# Main training loop\n# =========================================================\nWEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nInitializing {n_seg} SIREN networks (3->{HIDDEN}->{HIDDEN}->3)...\")\nparams = init_weights(n_seg, HIDDEN, OMEGA_0, device)\nW1, b1, W2, b2, W3, b3 = params\n\noptimizer = torch.optim.Adam(params, lr=LR)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=EPOCHS, eta_min=LR * 0.01)\n\nbest_loss = torch.full((n_seg,), float('inf'), device=device)\nbest_params = [p.detach().clone() for p in params]\n\nn_mini = max(1, SAMPLES // MINI_BATCH)\nprint(f\"Training: {EPOCHS} epochs, {SAMPLES} samples/seg, {n_mini} mini-batches\")\nprint(f\"Printing every {EVAL_EVERY} epochs\\n\")\n\nt_train = time.time()\nfor epoch in range(EPOCHS):\n    # GPU-native sampling — no CPU involved\n    coords_all, targets_all = sample_gpu(frames_gpu, n_seg, SAMPLES)\n\n    epoch_loss = 0.0\n    perm = torch.randperm(SAMPLES, device=device)\n\n    for mb in range(n_mini):\n        start = mb * MINI_BATCH\n        end = min(start + MINI_BATCH, SAMPLES)\n        idx = perm[start:end]\n        batch_coords = coords_all[:, idx]\n        batch_targets = targets_all[:, idx]\n        pred = batched_forward(batch_coords, params, OMEGA_0)\n        loss = ((pred - batch_targets) ** 2).mean()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    scheduler.step()\n    avg_loss = epoch_loss / n_mini\n\n    # Track best weights per segment\n    if (epoch + 1) % 100 == 0 or epoch == 0:\n        with torch.no_grad():\n            seg_mse = ((pred - batch_targets) ** 2).mean(dim=(1, 2))\n            improved = seg_mse < best_loss\n            if improved.any():\n                for i, p in enumerate(params):\n                    best_params[i][improved] = p[improved].detach()\n                best_loss[improved] = seg_mse[improved]\n\n    if (epoch + 1) % EVAL_EVERY == 0 or epoch == 0:\n        elapsed = time.time() - t_train\n        remaining = elapsed / (epoch + 1) * (EPOCHS - epoch - 1)\n        print(f\"epoch {epoch+1:5d}/{EPOCHS}: loss={avg_loss:.6f} \"\n              f\"lr={scheduler.get_last_lr()[0]:.2e} \"\n              f\"[{elapsed:.0f}s elapsed, ~{remaining:.0f}s remaining]\")\n        if (epoch + 1) % 500 == 0:\n            psnrs = evaluate_psnr(params, frames_gpu, OMEGA_0, device, max_segs=5)\n            avg_psnr = np.mean([p for _, p in psnrs])\n            print(f\"  PSNR (first 5 segs): {avg_psnr:.1f} dB\")\n\ntotal_time = time.time() - t_train\nprint(f\"\\nTraining done in {total_time:.0f}s ({total_time/60:.1f} min)\")\n\n# Restore best weights\nfor i, p in enumerate(params):\n    p.data.copy_(best_params[i])\n\n# Full PSNR evaluation\nprint(\"\\nEvaluating all segments...\")\nt_eval = time.time()\npsnrs = evaluate_psnr(params, frames_gpu, OMEGA_0, device)\neval_time = time.time() - t_eval\nall_psnr = [p for _, p in psnrs]\nprint(f\"PSNR: avg={np.mean(all_psnr):.1f}dB \"\n      f\"min={np.min(all_psnr):.1f}dB max={np.max(all_psnr):.1f}dB \"\n      f\"[{eval_time:.0f}s]\")\n\n# Export all segments\nprint(f\"\\nExporting {n_seg} segments...\")\nt_export = time.time()\nfor seg in range(n_seg):\n    export_segment_pt(params, seg, OMEGA_0)\n    export_segment_binary(params, seg, OMEGA_0)\nprint(f\"Exported in {time.time()-t_export:.1f}s\")\nprint(f\"  .pt files: {WEIGHTS_DIR}/segment_*.pt\")\nprint(f\"  .bin files: {WEIGHTS_DIR}/segment_*.bin\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Total: {n_seg} segments, {EPOCHS} epochs\")\nprint(f\"Time: {total_time:.0f}s training + {eval_time:.0f}s eval\")\nprint(f\"PSNR: {np.mean(all_psnr):.1f} dB average\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3: Zip weights and download\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create zip of all weight files\n",
    "shutil.make_archive('weights_h16', 'zip', '.', 'weights')\n",
    "print(f\"Created weights_h16.zip\")\n",
    "!ls -lh weights_h16.zip\n",
    "!echo \"Contents:\" && ls weights/ | head -10 && echo \"...\" && ls weights/ | wc -l && echo \"total files\"\n",
    "\n",
    "# Download\n",
    "files.download('weights_h16.zip')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}