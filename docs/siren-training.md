# Training and Deploying SIREN Networks

This document explains how to train a SIREN network in Python and deploy the weights to the Z7020 FPGA. The pipeline goes: procedural pattern → PyTorch training → Q4.28 quantization → Verilog include file → synthesis.

## Quick Start

```bash
# Train on the plasma pattern (default), export weights
python3 scripts/train_siren.py --pattern plasma --epochs 10000 --lr 5e-4

# Rebuild bitstream with MLP mode
source env.sh
vivado -mode batch -source vivado/run_all.tcl -tclargs 1

# Generate BOOT.bin and deploy
cd vivado && bootgen -image boot.bif -arch zynq -o BOOT.bin -w
# Copy BOOT.bin to SD card, power cycle
```

## How SIREN Training Works

### The idea

A SIREN is a function f(x, y, t) → (R, G, B) parameterized by weights. Training adjusts the weights so the function approximates a target pattern. Once trained, the FPGA evaluates f() for every pixel at every frame — the network generalizes and interpolates, producing smooth animation even between training samples.

### Network architecture

```
Input:  (x, y, t)  ← 3 values in [-1, +1]
        │
Layer 0: Linear(3→16) then sin(ω₀ · z)    ← ω₀ = 10 for first layer
        │
Layer 1: Linear(16→16) then sin(ω · z)     ← ω = 1.0 for hidden
        │
Layer 2: Linear(16→3) then sin(z)          ← output activation
        │
Output: (R, G, B)  ← 3 values in [-1, +1]
```

The omega (ω) parameters control frequency bandwidth:
- **ω₀ = 10**: First layer captures spatial frequencies up to ~10 cycles across the [-1,+1] input range
- **ω = 1.0**: Hidden layers refine without adding high-frequency noise

### SIREN weight initialization

Proper initialization is critical for SIREN convergence:

```python
# First layer: uniform(-1/fan_in, 1/fan_in)
# Hidden layers: uniform(-√(6/fan_in)/ω, √(6/fan_in)/ω)
```

This ensures the input to sin() has unit variance at initialization, preventing the network from starting in a saturated or degenerate state.

### Training targets

`train_siren.py` includes four procedural patterns:

| Pattern | Description | Typical loss |
|---------|-------------|-------------|
| `plasma` | Classic plasma effect with rotating sine waves | 0.033 |
| `lava_lamp` | Organic flowing blobs | 0.05-0.08 |
| `hsv_flow` | Smooth rainbow hue cycling | 0.15-0.18 |
| `reaction_diffusion` | RD-like interfering wave patterns | 0.06-0.10 |

Each pattern is a pure function `f(x, y, t) → (R, G, B)` with no learned state — the network learns to approximate it.

```bash
# Train on different patterns
python3 scripts/train_siren.py --pattern plasma
python3 scripts/train_siren.py --pattern lava_lamp
python3 scripts/train_siren.py --pattern hsv_flow
```

### Training data generation

The script samples a 48×48×24 grid of (x, y, t) points:
- x, y ∈ [-1, +1] (spatial)
- t ∈ [0, 4π] (temporal, ~2 full slow cycles)
- Total: 55,296 training samples

All coordinates and target colors are in [-1, +1].

## From Floats to Fixed-Point

### The quantization step

After training, weights are converted from float32 to Q4.28 signed integers:

```python
def float_to_q428(val):
    clamped = max(-8.0, min(val, 8.0 - 1/2**28))
    return int(round(clamped * 2**28))
```

Q4.28 has range [-8.0, +8.0) with resolution 3.73×10⁻⁹. This is more than enough for SIREN weights, which typically fall in [-7, +7] with ω₀=10.

### Omega baking

SIREN's forward pass computes sin(ω · (Wx + b)). The FPGA has no separate ω multiplication — it just computes sin(Wx + b). So the export script **bakes omega into the weights**:

```python
# During export:
w_exported = w_trained * omega
b_exported = b_trained * omega
```

This means the weights in `mlp_weights.vh` are pre-scaled. If ω₀ = 10, a first-layer weight of 0.3 becomes 3.0 in the exported file.

### Clipping check

The script reports if any weights exceed the Q4.28 range:

```
Weight range: [-6.22, 6.84]
WARNING: 0 weights clipped to Q4.28 range [-8, +8)
```

If clipping occurs, reduce ω₀. With ω₀=30, 13 weights were clipped; ω₀=10 eliminated all clipping.

## The Export Format

`mlp_weights.vh` is a Verilog include file with an `initial begin` block:

```verilog
// mlp_weights.vh — Auto-generated by train_siren.py
// Network: 3 -> 16 -> 16 -> 3 SIREN
// Total parameters: 387
// Weight range: [-6.2167, 6.8389]

initial begin
    weight_mem[  0] = 32'sh0262AE8C;  // L0 w[0][0] = +2.386789
    weight_mem[  1] = 32'shFD83F6B2;  // L0 w[0][1] = -2.484432
    weight_mem[  2] = 32'sh03A1CC78;  // L0 w[0][2] = +3.632005
    ...
    weight_mem[386] = 32'shFF12B40E;  // L2 b[2] = -0.928616
end
```

This is `include`d by `mlp_core.v` to initialize the weight BRAM. Each value is a Q4.28 signed 32-bit hex literal with a comment showing the original float value and its position in the network.

### Binary export (optional)

For runtime weight loading (not yet implemented on FPGA), the script can export raw binary:

```bash
python3 scripts/train_siren.py --binary scripts/weights.bin
```

This writes 387 × 4 = 1,548 bytes of little-endian Q4.28 values. A future version could load these from the SD card via a PS-side driver.

## Training Tips

### Omega selection

| ω₀ | Effect | Weight range | Notes |
|-----|--------|-------------|-------|
| 30 | High spatial frequency, sharp detail | [-20, +20] | Clips in Q4.28 |
| 10 | Medium frequency, smooth patterns | [-7, +7] | Good balance |
| 5 | Low frequency, very smooth blobs | [-3, +3] | May look bland |

Start with ω₀=10. Increase if the output looks too smooth; decrease if weights clip.

### Epochs and learning rate

```bash
# Quick test (30 seconds)
python3 scripts/train_siren.py --epochs 2000 --lr 1e-4

# Production quality (2-3 minutes)
python3 scripts/train_siren.py --epochs 10000 --lr 5e-4
```

Loss below 0.05 generally looks good. The network will never perfectly reproduce the target — it's a 387-parameter approximation of a function sampled at 55K points.

### Writing custom patterns

Add a function to `train_siren.py` that takes `(x, y, t)` numpy arrays and returns an `(N, 3)` array of RGB values in [-1, +1]:

```python
def target_my_pattern(x, y, t):
    r = np.sin(x * 5.0 + t) * 0.5
    g = np.cos(y * 3.0 - t * 0.7) * 0.5
    b = np.sin((x + y) * 4.0 + t * 1.2) * 0.5
    return np.stack([r, g, b], axis=-1)

PATTERNS['my_pattern'] = target_my_pattern
```

Then train with `--pattern my_pattern`.

Key constraints:
- Output must be in [-1, +1] (the FPGA maps this to [0, 2) for RGB565)
- Patterns should vary smoothly in t — abrupt changes won't interpolate well
- Include enough spatial variation to be visually interesting at 320×172

## Bad Apple Video Training

For video playback, each 10-frame segment gets its own SIREN network. The full Bad Apple video (6575 frames at 320x172) is split into 658 segments, each trained independently.

### Batched GPU training

`scripts/train_colab_batched.py` trains all 658 networks simultaneously using `torch.bmm` — one massive batched matrix multiply per layer instead of 658 sequential tiny ones. This achieves ~100x speedup over sequential training.

```bash
# On a T4 GPU via Colab (or any CUDA GPU)
python3 scripts/train_colab_batched.py --epochs 5000 --samples 50000 --mini-batch 10000
```

Or use the self-contained notebook `scripts/train_h16_colab.ipynb` which handles video upload, frame extraction, training, and weight download.

### Aspect ratio

The display is 320x172 (not square). Training coordinates use:
- x: [-1, +1]
- y: [-0.5375, +0.5375] (i.e. ASPECT_Y = 172/320)

This must match the FPGA's coordinate generation in `pixel_scheduler.v`. A mismatch causes spatial distortion.

### FPGA validation

Use `scripts/fpga_sim.py` to validate weights against the FPGA's fixed-point datapath before deploying:

```bash
python3 scripts/test_mlp_render.py  # Renders frame using fpga_sim Q4.28 pipeline
```

The float PSNR and FPGA PSNR typically differ by 1-5 dB due to Q4.28 quantization and sine LUT approximation.

## Deployment Checklist

1. Train: `python3 scripts/train_siren.py --pattern plasma --epochs 10000 --lr 5e-4`
2. Check output: loss < 0.05, zero clipped weights
3. Verify `rtl/mlp_weights.vh` was updated (check timestamp)
4. Build with MLP mode: `vivado -mode batch -source vivado/run_all.tcl -tclargs 1`
5. Check build: WNS > 0, no timing violations
6. Generate BOOT.bin: `cd vivado && bootgen -image boot.bif -arch zynq -o BOOT.bin -w`
7. Deploy: copy to SD card, power cycle
8. Verify: display shows smooth animated colors (not thresholded bands)

### Common problems

| Symptom | Cause | Fix |
|---------|-------|-----|
| Static image, no animation | Time scaling too slow | Increase shift in `time_val` |
| Pure R/G/B bands, no mixing | Bit extraction bug | Verify `rs[28:24]` not `rs[27:23]` |
| Visible frame rendering | No double buffering | Check `disp_fb_0/disp_fb_1` in MLP mode |
| Weights clipped warning | ω₀ too high | Reduce omega_0 (try 10) |
| Mandelbrot appears instead | Wrong COMPUTE_MODE | Pass `1` to run_all.tcl: `-tclargs 1` |

## How the FPGA Differs from PyTorch

| Aspect | PyTorch (training) | FPGA (inference) |
|--------|-------------------|------------------|
| Precision | float32 | Q4.28 fixed-point |
| sin() | IEEE 754 libm | 256-entry quarter-wave LUT |
| Multiply | FP multiply unit | DSP48E1 (25×18 decomposed) |
| Parallelism | Batch over samples | 18 cores over pixels |
| Latency per pixel | ~1 µs (GPU) | ~12 µs (50 MHz) |
| Throughput | Millions/sec (GPU) | 1.46M/sec (18 cores) |
| Power | ~200W (GPU) | ~2W (Zynq PL) |

The quantization from float32 to Q4.28 introduces small errors (~10⁻⁸ per operation). Over 3 layers with 16-wide dot products, these accumulate but remain visually imperceptible. The sine LUT's 256-entry resolution (1/1024 of a full cycle per step) adds further approximation, but sin() varies slowly enough that 8-bit indexing is sufficient.
